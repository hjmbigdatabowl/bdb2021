---
title: "Submission"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Submission}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE
)
```

```{r setup, include=F}
library(bdb2021)
```

## Introduction

The objective of our project is to measure the ability of defensive backs at different aspects of their defensive performance: deterring targets (either by reputation or through good positioning), closing down receivers, and breaking up passes. We do this by fitting four models, two for predicting the probability that a receiver will be targeted and two for predicting the probability that a pass will be caught, which we then use to aggregate the contributions of defensive backs of the course of the season.

## Modeling Framework

We opted to use XGBoost for all of our models. At a high level, the reason for this choice was the ability of a tree booster to find complex interactions between predictors, which we expected there to be many of. Tree boosting also allows for null values to be present, which was helpful in how we divvied up credit in the catch probability models (more on that later). Finally, tree boosting is a relatively simple, easy to tune algorithm that also tends to perform extremely well, as was the case for us.

## Catch Probability

Our catch probability model has two distinct components: The catch probability at throw time -- as in, the chance that the pass is caught at the time the quarterback releases the ball -- and the catch probability at arrival time -- the chance the pass is caught at the time the ball arrives. These probabilities are clearly distinct, since a lot can happen between throw release and throw arrival. First, I'll walk through the features that are used in building each model. The sets are very similar. 

For the throw time model, which I might refer to as just the "throw" model, while the arrival time model is the "arrival" model, the most important predictors are things you might expect: the distance of the receiver to the closest defender, the position of the receiver in the $y$ direction (i.e. distance from the sideline), the distance of the throw, the position of the football in the $y$ direction at arrival time (this is mostly catching throws to the sideline and throw aways), the velocity of the throw, the velocity and acceleration of the targeted receiver, and a composite receiver skill metric. See the appendix for a full table of features. 

These two models both perform quite well, and far better than random chance. We achieve **INSERT METRICS HERE (F, AUC for each)** Below are plots of the calibration of the predictions of each of the models on a held out data set.

&nbsp;


```{r, echo=FALSE,out.width="40%", out.height="30%",fig.cap="Catch Probability Model Calibration Plots",fig.show='hold',fig.align='center'}
par(mfrow = c(1,2))
knitr::include_graphics(c("../inst/plots/calplot_t.png", "../inst/plots/calplot_a.png"))
```

&nbsp;

We can do a few particularly interesting things with the predictions from these two models in tandem. Namely, we can use the two to calculate marginal effects of the play of the defensive backs. A simple example is as follows: Imagine a throw with a catch probability of $.5$. If that throw is dropped, we should somehow credit the defense with $.5$ catches, which is the difference between $.5$, the prediction, and $0$, the true outcome. We can extend this approach into a two-step process by using the two models together. First, we can calculate the difference between the catch probability at throw time and at catch time in order to give credit to the defense for closing down a receiver after the throw has been made. In other words, if a throw has catch probability $.8$ at throw time, and that goes down to $.4$ at catch time, then we should credit the defense with $.4$ drops added from closing the receiver down. Then, we can use the same logic to credit the defense for breaking up plays by taking the difference between the catch probability at arrival time and the true outcome.

However, we need to come up with a clever way to divvy up the credit. In the previous example where we have to credit the defense with $.5$ drops added, *who* exactly on the defense do we give that credit to? There are a couple of heuristics that might make sense. One option would be to just split it up evenly amongst the defense, but this is a bad heuristic because clearly some defenders will have more of an impact on a pass being caught than others. We might also give all of the credit to the nearest defenders, but that would be unfair to players who are within half a yard of the play and are also affecting its outcome, but get no credit. Instead, we opt to use the model to engineer the credit each player deserves by seeing how the catch probabilities would change if we magically removed them from the field. To do this, we take each player off of the field and re-run the predictions, and see how big the magnitude of the change in catch probability is. The bigger the magnitude difference, the more credit that player gets. So if one player gets 75% of the credit for a play and the play is worth $.5$ drops added, then that player gets $.5 \cdot .75 = .375$ drops of credit, and the remaining $.125$ drops is divvied up amongst the other defenders in the same fashion.

## Target Probability


## Further Work

There are obviously a number of things that we didn't consider or that would be interesting extensions of this project. Two obvious ones are interceptions added and fumbles added, as those are hugely impactful football plays. We also only considered raw changes (i.e. targets prevented and drops added), but using EPA added instead would certainly be a better metric. It would also be interesting to apply these models to more seasons of data to track player trajectories and to see how consistent they are across seasons.

## Appendix
