---
title: "Submission"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Submission}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE
)
```

```{r setup, include=F}
library(bdb2021)
```

## Introduction

Our project aims to measure the ability of defensive backs at different aspects of their defensive performance: deterring targets (either by reputation or through good positioning), closing down receivers, and breaking up passes. We do this by fitting four models, two for predicting the probability that a receiver will be targeted and two for predicting the probability that a pass will be caught, which we then use to aggregate the contributions of defensive backs of the course of the season.

## Modeling Framework

We opted to use XGBoost for all of our models. At a high level, we chose a tree booster for its ability to find complex interactions between predictors, something we anticipated would be necessary for this project. Tree boosting also allows for null values to be present, which helped us divvy up credit in the catch probability models (more on that later). Finally, tree boosting is a relatively simple, easy to tune algorithm that also tends to perform extremely well, as was the case for us.

## Catch Probability

Our catch probability model has two distinct components: The catch probability at throw time -- as in, the chance that the pass is caught at the time the quarterback releases the ball -- and the catch probability at arrival time -- the chance the pass is caught at the time the ball arrives. These probabilities are clearly distinct, since a lot can happen between throw release and throw arrival. First, we will walk through the features that are used in building each model. 

For the throw time model (which we will refer to as the "throw" model) and the arrival time model (the "arrival" model), the most important predictors were what we expected entering this project: the distance of the receiver to the closest defender, the position of the receiver in the $y$ direction (i.e. distance from the sideline), the distance of the throw, the position of the football in the $y$ direction at arrival time (this is mostly catching throws to the sideline and throw aways), the velocity of the throw, the velocity and acceleration of the targeted receiver, and a composite receiver skill metric. See the appendix for a full table of features. 

These two models both perform quite well, and far better than random chance. The throw model accurately predicts $74\%$ of all passes, with strong precision ($84\%$) and recall ($76\%$), and an AUC of $.81$. As can be expected, our arrival model outperforms the throw model in all measures - accurately predicting 78% of all passes with a precision of 88%, a recall of 79%, and an AUC of 87%. Below are plots of the calibration of the predictions of each of the models on a held out data set.

&nbsp;


```{r, echo=FALSE,out.width="45%", out.height="45%",fig.cap="Catch Probability Model Calibration Plots",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("../inst/plots/calplot_t.png", "../inst/plots/calplot_a.png"))
```

&nbsp;

We can do a few particularly interesting things with the predictions from these two models in tandem. Namely, we can use the two to calculate marginal effects of the play of the defensive backs. A simple example is as follows: For a given pass attempt, our throw model estimates that there is a $.8$ chance of a catch. By the time that pass arrives, however, our arrival model estimates instead that there is a $.5$ chance of a catch. Ultimately, the play results in a drop. In total, our defense can get credit for $.8$ drops, but we can break it down into $.3$ drops worth from closing on the receiver and $.5$ drops from breaking up the play.

The main challenge comes not from this assignment, but from the distribution of credit *among* the defenders. In the previous example where we have to credit the defense with $.8$ drops added, *who* exactly on the defense do we give that credit to? There are a couple of heuristics that might make sense. One option would be to just split it up evenly amongst the defense, but this would be a bad heuristic because some defenders will have more of an impact on a pass being caught than others, and thus deserve more credit. We might also give all of the credit to the nearest defender, but that would be unfair to players who are within half a yard of the play and are also affecting its outcome, but get no credit. We settled on to use the model to engineer the credit each player deserves by seeing how the catch probabilities would change if we magically removed them from the field. To do this, we take each player off of the field and re-run the predictions, and see how big the magnitude of the change in catch probability is. The bigger the magnitude difference, the more credit that player gets. So if one player gets 75% of the credit for a play and the play is worth $.8$ drops added, then that player gets $.8 \cdot .75 = .6$ drops of credit, and the remaining $.2$ drops is divvied up amongst the other defenders in the same fashion.

## Target Probability

Our target model is based around comparing the probabilities a receiver is targeted before the play begins and when the ball is thrown with the actual receiver targeted. We can use these to make estimates of how well the defender is covering (are receivers less likely to be thrown the ball because of the defensive back's pre-throw work?) and how much respect they get from opposing offenses (do QBs tend to make different decisions at throw time because of the defensive back?). 

To determine the probability of a receiver being targeted before the play, we chose to take a naive approach. Each receiver on the field is assigned a "target rate" of  $\frac{targets}{\sqrt{plays}}$, which is then adjusted for the other receivers on the field and used as the only feature for a logit model. The idea of this rate was to construct a statistic which rewarded receivers for showing high target rates over a large sample without relying solely on volume.

The model for target probability at the time of throw was a tree booster similar to the two catch probability models. This model used the positional data, comparing the receiver position to the QB and the three closest defenders along with a variety of situational factors such as the distance to the first down line, time left, weather conditions, how open that receiver is relative to others on the play to determine how likely that receiver is to be targeted. 

**some discussion of model metrics!**

We can estimate how a defensive back is impacting the decisions made by the QB through two effects. The first, comparing the target probablitiy prior to the play to the target probability at the time of the throw, is meant to estimate how well the receiver is covered on the play. For example, consider a receiver with a target probability of $0.2$ prior to the snap who ends up open enough to get a target probability of $0.5$ when the ball is thrown. This difference is attributed to the closest defensive back who would be credited with $-0.3$ coverage targets. If on the same play another receiver had a pre-snap target probability $0.3$ but a target probability of $0$ at throw time, the closest defensive back would be credited with $+0.3$ coverage targets. The other effect attempts to measure how the QB is deterred from throwing when that particular defensive back is in the area of the receiver by comparing the probability of a target to the actual result. So if a certain receiver has a target probability of $0.6$ at the time of the throw and isn't targeted, the closest defender is credited with $+0.6$ deterrence targets.

## Further Work

There are obviously a number of things that we didn't consider or that would be interesting extensions of this project. Two obvious ones are interceptions added and fumbles added, as those are hugely impactful football plays. We also only considered raw changes (i.e. targets prevented and drops added), but using EPA added instead would certainly be a better metric. It would also be interesting to test how similar our metrics are between seasons to confirm that what we're measuring is a stable skill of the defender.

## Appendix
